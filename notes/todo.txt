- Complete Python string parsing

- Complete shell string parsing (termination on things other than EOL
  and whiteapce).

- Debugging & verbosity level

----------------------------------------------------------------------

- Organize internal ops -- Fork, LabelThread

- How do exit codes fit in?

----------------------------------------------------------------------

Provide a way to specify farcel config path. Maybe as a property of
the cluster.

----------------------------------------------------------------------

Distinct color for broken symlink.

----------------------------------------------------------------------

Tab completion:

- Tab completion for command should distinguish between first op of
  pipeline and subsequent ones. I.e., only show those ops that can be
  used in the current position within the pipeline.

----------------------------------------------------------------------

Add notion of current job, and then allow bg/fg ops to omit args.

----------------------------------------------------------------------

Would multiprocessing.Process provide better streaming than
subprocess.Popen? I.e., not waiting for op to complete.

----------------------------------------------------------------------

Should help output go through more?

----------------------------------------------------------------------

Configuration:

- Look in ~, /etc/marcel.

- Be able to specify config file (or multiple locations) in startup.

----------------------------------------------------------------------

History commands

- If history were in the namespace, then edited_command wouldn't need
  special handling.

----------------------------------------------------------------------

Not all ops can be used in API, e.g. edit, help. What about fork? remote?

----------------------------------------------------------------------

API needs documentation. help? HTML?

----------------------------------------------------------------------

Exit codes? (Not just an api issue)

----------------------------------------------------------------------

window: what is supposed to happen if overlap = 0? disjoint = 0? Not tested.
test types other than int

----------------------------------------------------------------------

sudo has a zillion args. How to express this on sudo()? **kwargs?

----------------------------------------------------------------------

first constructs an Exception out of an Error.

Is it feasible to have Error carry the original exception, and reraise
it, or at least an exception of the same type? What if the Error was
remote?

----------------------------------------------------------------------

ls spacing is too wide. How to pick a width? Buffer results? Don't
buffer and adapt?

----------------------------------------------------------------------

This is broken: Op.op_name() evalutes to "op".

    @staticmethod
    def check_arg(ok, arg, message):
        if not ok:
            cause = (f'Incorrect usage of {Op.op_name()}: {message}'
                     if arg is None else
                     f'Incorrect value for {arg} argument of {Op.op_name()}: {message}')
            raise marcel.exception.KillCommandException(cause)

----------------------------------------------------------------------

ps lines are often long enough to wrap. Should render_full leave off
args? If so, then provide an args method.

----------------------------------------------------------------------

What if there are two shells running at the same time -- how is
history file maintained? Probably lose updates from the first one to
exit. How should this be handled? To be safe, on exit, should read,
append, write. Atomically.

----------------------------------------------------------------------

Should jobs and commands be objects? That would allow for better
formatting.

----------------------------------------------------------------------

Should {...} work as a glob pattern? It does in bash. pathlib.Path.glob
doesn't.

ls -fr ~/git/marcel/{marcel,test} \
| select (f: f.suffix == '.py') \
| map (f: f.readlines()) \
| expand \
| red count
No qualifying paths: ['~/git/marcel/{marcel,test}']

----------------------------------------------------------------------

cat = [ map (f: (f, f.readlines())) | expand 1 ]
(cat)

prints:

    pipeline(map(f: (f, f.readlines())) | expand(1))

It would be nice to have the original source.

----------------------------------------------------------------------

These commands do different things:

    ls -fr **/*.py
    ls -fr | select (f: f.suffix == '.py')

The first one avoids symlinks (or symlinks to visited directories? or
files?). The second one explores both paths.

----------------------------------------------------------------------

I keep forgetting to set pipeline's error handler. Could be done by
Pipeline.copy.

----------------------------------------------------------------------

env has paths as strings. Should be Paths.

----------------------------------------------------------------------

ls API: Need to complain if depth is other than 0 or 1

----------------------------------------------------------------------

Exhaustive type error testing in API?

----------------------------------------------------------------------

stack traces: Include them, but have print_stack check a flag to
determine if they should really be printed.

----------------------------------------------------------------------

TestBase.reset_environment does too much. Move everything but Main
construction into subclasses.

----------------------------------------------------------------------

Controlling Popen processes:

https://pymotw.com/2/subprocess/#process-groups-sessions

----------------------------------------------------------------------

Pipelines:

- In parser, the create_op_variable check is useless. The variable
  value could change before execution, and may even cease to
  exist. Make create_op_variable the last resort, and then at runtime,
  complain if the var isn't defined, or if it's value is of the wrong
  type.

- Allow [...] to delimit a pipeline, even when not necessary. (Like {
  ... } around single statement if, in C.)


Should assign allow assignment of pipeline without brackets?

----------------------------------------------------------------------

Should a var hide an op by the same name?

----------------------------------------------------------------------

Oops, [] is overloaded:

M-0.9.17 jao@cheese:/tmp/csv$ ls [b-f]*.csv
Operator ls: filenames must be a string: [runpipeline(b-f)]

But escaping works:

M-0.10.6 jao@cheese:~$ ls \[p-r\]*
-rw-rw-r--   jao    jao       36883   2020 Jul 05 19:23:14   passwords.txt
-rw-r--r--   jao    jao       13377   2019 Jan 20 12:43:37   reality_distortion_field.md
-rw-rw-r--   jao    jao        2415   2019 Oct 06 23:13:49   reload.txt


----------------------------------------------------------------------

Can't run ssh!

----------------------------------------------------------------------

EDITOR set to host's EDITOR on startup. Which is convenient, but odd
if host value changes. How to keep the two in sync? Maybe reporting
EDITOR should always get the value of os.getenv('EDITOR')?

----------------------------------------------------------------------

Nushell uses $it for current pipeline item. Not a bad
idea. Alternative to args (without --all).

----------------------------------------------------------------------

Tab completion:

- We often know when an op ends. Can tab-complete for |, >, >>, [, ], Enter.

- We sometimes know when an arg is not a flag. Can prompt for var, (, [.

----------------------------------------------------------------------

Help needs to discuss >, >>, pipeline params.

help pipeline, examples summing filesizes are inconsistent. Unclear
that early ones compute local sum of sizes, and later ones are global.

----------------------------------------------------------------------

In api functions, why [None] instead of []? E.g., in store()

----------------------------------------------------------------------

Operator and pipeline logging, easily controllable, is needed. Can
this be done avoiding runtime penalty when logging not in use?

----------------------------------------------------------------------

ls abc.{x,y}

is not parsed properly, because the comma is its own token.

Could fix this by recognizing COMMA only in the right context. (Lexer
would need to know Parser state.)

OR: in the context that a comma is used, look for string instead, and
check that it is a comma.

----------------------------------------------------------------------

Arg checking (derived from bug 129):

Doing parse-time checking isn't sufficient. A check is also needed at
execution time, for args that need to be evaluated. The conversion
functions in ArgsParser (e.g. str_to_int) don't really allow for that
second use, as they are tied to the ArgsParser object, which isn't
available at runtime. They could *almost* be made independent of
ArgsParser, but there are some dependencies:

- self.op_name: op isn't available during arg parsing. 

- self.env

- self.current_op: For function(). Need to revisit error handling
  (which is why f.set_op(self.current_op) is called), and perhaps get
  rid of this call.

----------------------------------------------------------------------

Tab completion for env vars.

----------------------------------------------------------------------

Can threading/multiprocessing (for fork) be replaced by async io?

https://realpython.com/async-io-python/

And maybe for the bash op:

- create_subprocess_exec
- create_subprocess_shell

----------------------------------------------------------------------

Documentation for builtin functions

----------------------------------------------------------------------

Osh handled stdin:

    osh ^ ...

Need something like this for marcel.

The read op (without a FILENAME) assumes that inputs are File objects,
not a stream of text.

What about connecting stdin to input stream? If nothing shows up, no
harm. As long as we don't get stuck waiting forever.

----------------------------------------------------------------------

json

https://cameronnokes.com/blog/working-with-json-in-bash-using-jq/

----------------------------------------------------------------------

Unbundle csv/tsv/pickle options from read and write?

This:

    read --csv foobar

would become:

    read foobar | parse --csv

or 

    foobar > parse --csv

This:

    ... | write --csv foobar

would become:

    ... | format --csv | write foobar

or

    ... | format --csv > foobar

----------------------------------------------------------------------

File copying to/from cluster

    upload path cluster:remote_path

    download cluster:remote_path path

Or a single command, where direction depends on where the cluster arg
appears?

Cluster is now an Object. Say so in "help object", and add "help
cluster".

......................................................................

upload FILENAME ... CLUSTER DIR 

    FILENAME     A filename or glob pattern.

    CLUSTER      A marcel Cluster.

    PATH         Path to a directory on CLUSTER.

Copies local files to the indicated directory on each node of a cluster.

The files to be copied are specified by one or more FILENAMEs. Each
FILENAME is a file name or a glob pattern. 

CLUSTER must be configured for marcel, (run "help cluster" for
information on configuring clusters).

PATH must be an absolute path, corresponding to a pre-exising
directory on each node of the CLUSTER. The user configured for cluster
access must have permission to write to the directory.

......................................................................

download [-s|--separate] CLUSTER FILENAME ... DIR

----------------------------------------------------------------------

rsync -avz foo* bar/* user@host:/abc/xyz

    @cluster [rsync ... (HOST):/abc/xyz]   # RUNS HERE

    @cluster [gen 3]   # RUNS THERE

Alternative approach for rsync:

    args CLUSTER [host: rsync ... (host):/abc/xyz]

But args isn't quite right. It doesn't parallelize, and is geared to
iteration over input stream.

Upload:

    fork CLUSTER [host: rsync ... (host):/abc/xyz]

Download:

    fork CLUSTER [host: rsync (host):/abc/xyz/foo* ./download/(host)]

First argument to fork is something iterable. The things iterated over
are bound to the pipeline arg.  As long as CLUSTER is bound to a
Cluster, then this will just work as long as Cluster is implemented to
be an iterator.

......................................................................

- Get rid of Environment.cluster(self, name). Fork should call getvar
  instead, and then ensure that the returned value is iterable. (Use
  iter() to get the iterator, or raise TypeError).

----------------------------------------------------------------------

{L,wrap=F}fork FORK_GEN PIPELINE

{L,indent=4:28}{r:FORK_GEN}                     An int or iterable used to specify forks.
   
{L,indent=4:28}{r:PIPELINE}                     A pipeline whose instances are to be executed concurrently.

Instances of a pipeline are executed concurrently.

{r:FORK_GEN} is an int or an iterable, used to specify the number of
concurrent executions of {r:PIPELINE}.  For discussion purposes, the
execution of each instance of {r:PIPELINE} is executed by a thread, but
the actual implementation is unspecified. {r:PIPELINE} has one argument, a
thread id. Output from fork comprises tuples containing the thread id,
and the output from the pipeline.

Example:

{p,indent=4,wrap=F}
fork 3 [id: gen (id+1) 100]

This command creates three threads, with ids 0, 1, 2. For id 0, the
command executed is gen 1 100, which generates the stream [100]. For
id 1, the stream is [100, 101], and for id 2, the stream is [100, 101,
102]. The fork command prepends the id to each tuple of output from
the gen operator, so the output for the command is something like
this:

{p,indent=4,wrap=F}
(0, 100)
(1, 100)
(2, 100)
(1, 101)
(2, 101)
(2, 102)

(Any interleaving of the streams may be observed.)

If {r:FORK_GEN} is an iterable, then the thread id is bound to each value
returned by the iterable. So this command:

{p,indent=4,wrap=F}
fork 'abc' [id: gen 3 100]

yields:

{p,indent=4,wrap=F}
('a', 100)
('b', 100)
('c', 100)
('a', 101)
('b', 101)
('c', 101)
('a', 102)
('b', 102)
('c', 102)

The FORK_SPEC may be a {n:Cluster}. A {n:Cluster} is iterable, and the values
obtained by iteration are {n:Host} objects.

It should not normally be necessary to use a {n:Cluster} for
{r:FORK_GEN}. Remote execution can be done by using special syntax,
e.g. to send a grep command to each node in cluster lab:

{p,indent=4,wrap=F}
@lab [grep foobar /var/log/syslog]

This is equivalent to something like:

{p,indent=4,wrap=F}
fork lab [host: ssh -i ~/.ssh/(host).pem -l (host.cluster.user) "grep foobar /var/log/syslog]

Similarly, copying files to and from clusters can be done using the
{r:fork} command (e.g., with the {r:PIPELINE} using {n:rsync}), but it would be
far simpler to use the {n:upload} and {n:download} commands.

......................................................................

- Add pipeline args to fork.

- upload

- download

- Document Cluster and Host as objects (update object doc too).

- Add fork as op to top-level help.

+ Remote is internal only. Don't need remote() in API. 

+ Remote class could be demoted to be nested, like ForkWorker.SendToParent.

+ LabelThread can also be demoted.
