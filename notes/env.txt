Getting the environment where it needs to be has been the source of a
lot of pain.

Where it needs to be:

- Available to every pipeline.
- Available to every op.
- All of the above, across jobs (i.e. different processes)/
- Available to every args parser.
- parser.
- main.
- Function's __globals__.
- All of the above (except parsing) for remote execution.

Having *copies* of the Environment (as for processes) is problematic,
because changes to env vars, definitions of new ones, deletions, need
to be propagated back to main's Env. Hence Env.changes, modified_vars
and so on.

Bug 109 shows that copying Envs can be problematic for
performance. The fix to 109 is to minimize the namespace of a
function, but that's not a complete fix. The namespace of a function
might include a list of 100K Files, for example, because the function
really does refer to that variable.

Alternative:

- multiprocessing.managers can be used to create an
  EnvironmentManager.

- Everything local (including jobs) could interact via messages.

- References to a list of 100K files would still be expensive, but
  then the cost is incurred only when it has to be.

- Two problems:

  1. Getting this to work for functions. It should be possible to
  implement an adapter to EnvironmentManager that has a dict
  interface. 

  2. Remote access: Probably want to materialize the Environment state
  and pass the copy. Unclear how to know when each kind of Environment
  copy (ref to manager, copy) is appropriate.

----------------------------------------------------------------------

The bug 109 partial fix isn't working, (see discussion in the bug
report).

Another idea: Moved saved streams "out of band", shared memory or a
file. Python pickling, protocol 5 seems to do this, but it is pretty
new; PEP 574 which explains it, is unclear on usage, and there doesn't
seem to be much experience with it. So roll your own:

- Stream class.

- Two reps: in a file, and in memory. The memory copy can be flushed
  anytime, e.g. on copying.

- __get/setstate__ would need to know if the copy is local or remote.

  local: copy just the file handle.
  remote: copy just the memory copy. In setstate, write to a new file.

- On reading (e.g. x > ...), use the memory copy if available,
  otherwise, the file copy.

......................................................................

2023 09 06

I think 109 was fixed by using Reservoirs for vars.

----------------------------------------------------------------------

2023 09 06

See bug 212. Ops owning Environment is a bad idea. Need to make
Environment available during execution (setup, run), but not stored.

Ops store the environment as Op._env, and access it via Op.env().

env() usage:

setup()

- Pass to Op.pipeline_arg_value(), which uses env to lookup arg value.

- Env passed to PipelineWrapper.create. 

- Bash, uses env to check if bash command is interactive (is_interactive_executable)

- Cd

- Download: Pass to Filenames() which needs to know pwd.

- FilenamesOp: pwd used twice.

- Edit

- Help

- History

- Load

...

run()

- Assign

- Cd

- Dirs

- Edit

- Env

- Import

- Popd

- Pushd

- Pwd

...

other:

- Call APIOp error_handler. env unused? 
  *** Check uses of error_handler

- PipelineWrapper.create and customize_pipeline callback.

- ForkManager and customize_pipeline callback.

- RunPipeline.receive, to push/pop scope.

......................................................................

Command.execute has env and pipeline. Execute can pass env to pipeline.setup and run.

Can propagate env via Op.run -> Op.send -> Op.receive.

That's kind of a mess.

Could also store Environment in Op._env, but temporarily, during
execution. But then it's present during serialization which is what
we're trying to avoid!

......................................................................

Add env to:

- Pipeline.handle_error, if needed


Remove env from API functions (like gen())

OpModule:
- _env needed?
- env() needed?

pos() is defined as lambda: env.current_op.pos(). It's going to break.
